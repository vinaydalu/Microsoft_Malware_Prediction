![malware](https://github.com/vinaydalu/Microsoft_Malware_Prediction/blob/main/Images/malware.jpg)
# <u> Microsoft Malware Prediction - Capstone project
*The malware industry continues to be a well-organized, well-funded market dedicated to evading traditional security measures. Once a computer is infected by malware, criminals can hurt consumers and enterprises in many ways.
With more than one billion enterprise and consumer customers, Microsoft takes this problem very seriously and is deeply invested in improving security. To develop techniques to predict if a machine will soon be hit with malware.*
# <u> 1. Data Description
To predict a Windows machine’s probability of getting infected by various families of malware, based on different properties of that machine. The telemetry data containing these properties and the machine infections was generated by combining heartbeat and threat reports collected by Microsoft's endpoint protection solution, Windows Defender.

Each row in this dataset corresponds to a machine, uniquely identified by a MachineIdentifier. HasDetections is the ground truth and indicates that Malware was detected on the machine. Using the information and labels in train.csv, you must predict the value for HasDetections for each machine in test.csv.

The sampling methodology used to create this dataset was designed to meet certain business constraints, both in regards to user privacy as well as the time period during which the machine was running. Malware detection is inherently a time-series problem, but it is made complicated by the introduction of new machines, machines that come online and offline, machines that receive patches, machines that receive new operating systems, etc. While the dataset provided here has been roughly split by time, the complications and sampling requirements mentioned above may mean you may see imperfect agreement between your cross validation, public, and private scores! Additionally, this dataset is not representative of Microsoft customers’ machines in the wild; it has been sampled to include a much larger proportion of malware machines.To import/view the dataset click the link below.
* [Dataset](https://www.kaggle.com/c/microsoft-malware-prediction/data)
# <u> 2. Data Wrangling
   ## <u> 2.1 Introduction
  This step focuses on collecting our data, organizing it, and making sure it's well defined. Paying attention to these tasks will pay off greatly later on. Some data cleaning can be done at this stage, but it's important not to be overzealous in your cleaning before you've explored the data to better understand it.
## <u> 2.2 Objectives
  * Before we start cleaning we must better understand what is in our data, which will inform how we want to analyze it.
  * Organizing the data, which is necessary because raw data comes in many different shapes and sizes.
  * Removing outliers and filling missing values.
## <u> 2.3 Data cleaning
  Because of the manganomonity of the data, my system facing issues to load the dataset. So to reduce the loading time we tried using dtype while reading the csv file. We have nearly 9 million rows where each row represents a system that runs on windows os. And it has totally 83 features (columns) that specify entire information about systems. We have a target variable, HasDetections which is a boolean. If it’s 1 - malware is in the system else no malware.
  
  Due to more features and rows, It’s better to get rid of those columns that won’t help our prediction model. Started by checking the number of null values in each column and calculated null values percentages per column.
  
  ![missing values](https://github.com/kiranmaiboda/Microsoft-Malware-Prediction/blob/main/images/missing%20values.png)
  
  Dropped all those features whose null values percentage was more than 60%.Then checked for high cardinality columns and dropped them too.Pandas profiling package helped in dropping those columns as it provided a report on each column. Left with 75 features, the next target is to eliminate nulls from the remaining features. It was handled by imputing most frequent values in categorical and mean in numerical columns by mostly mean or mode.
  ## <u> 2.4 Correlation Matrix
  Using the cleaned data frame and selecting numeric features from it, generated a correlation matrix.
  
  ![corr](https://github.com/vinaydalu/Microsoft_Malware_Prediction/blob/main/Images/correlation_image.png)
  
 1. Found out the correlation of features with target variable, and sorted the values in descending order, and it can be seen from above results that AVproductsstateIdentifier highly correlated with the target variable in terms of positive strength and AVProductsInstalled in terms of negative strength followed by CensusTotalPhysicalRAM and ISProtected etc in terms of positive strength and Census_IsAlwaysonALwaysConnectedCapable and so on in terms of negative strength
2. Using above results, it will be useful for creating new features or feature engineering from above given features. Like for example as AVproductsstateIdentifier and AvproductsInstalled are highly correlated with target variable then using those features we can create new feature using feature engineering hack.
  # <u> 3. Exploratory Data Analysis (EDA)
  We are left with 75 features and nearly 9 million rows to understand the data and bring out some insights in a presentable format. As we know we can visualise all types of columns in the same way. I have decided to segregate features into 3 types, Boolean, Numeric and Categorical data. By doing so we left with 17 - Boolean columns, 37 - Categorical columns and 18 - Numerical columns.
  ## <u> 3.1 Boolean columns
  As it has only 2 outputs and our target variable also has only 2 outputs. So, Came up with a visualisation technique where we first split using the target variable and then split on the boolean column to get insights. Let’s see an example of column IsBeta which can be On(1) or Off(0). IsBeta functionality is that if we enable it, windows will helps to keep your computer protected from current threats are automatically downloaded and protect from malware attacks.
  
  ![IsBeta](https://github.com/vinaydalu/Microsoft_Malware_Prediction/blob/main/Images/IsBeta.png)
  
  We can interpret this as follows: When IsBeta is not enabled, Malware attacks are equally distributed informing that IsBeta - 0 is not affecting the target variable. But, then the table below is telling a different story. We see a significant decrease in Malware Detected values, To be precise it has fallen from 99 to  0.000673%. This infers that IsBeta when enabled, reduces the chance of being attacked by malware.After visualizing the bargraphs of boolean columns with respect to target variable we found out that IsBeta, IsSxsPassiveMode, HasTpm, AutoSampleOptIn and Smode when these features are disabled they have highest percentage of Malware Detected.
  ## <u> 3.2 Categorical Columns
  The visualisation is similar to boolean but there are more than 2 categories unlike booleans. So, divided the categories into 2 types namely Most Vulnerable categories and
Safest Categories in each column. Let me explain with an example. Take column “ProductName” which is a Microsoft Defender anti malware software and visualise it.
  
  ![win8defender](https://github.com/vinaydalu/Microsoft_Malware_Prediction/blob/main/Images/productName.png)
  
  After analysing the bar graphs, table cointaining percentage of malware detected for each category in each category column . Some of these fatures are 
  
  Feature Name  |  Feature Value
  --------------|----------------
  ProductName   | win8defender
  Census_PowerPlatformRoleName|Mobile
  Census_PrimaryDiskTypeName|HDD                       Census_DeviceFamily|Windows.Desktop, 
  SmartScreen|requiredmin,                               Processor|X64
  Census_FlightRing|Retail                             Platform|Windows10
  Census_GenuineStateName|ISGenuine                     Sversion|10.0.0.0
  Census_OSArchitecture|amd64  
  
  these are the columns and their respective categories having highest percentage of malware detected
  ## <u> 3.3 Numerical columns
  Next comes the numeric columns, for this I puzzled myself whether to check the bi variant or distribution but distribution of violin and histogram were helpful to see some insights.
  
  ![num](https://github.com/vinaydalu/Microsoft_Malware_Prediction/blob/main/Images/UacLuaenable.png)
  
  After analysing the above dataframe we came to the conclusion that columns UacLuaenable, AVProductsEnabled, AVProductsStateIdentifier, RTPStateBitField, OrganisationIdentifier, AVProductsStatesIdentifier, AVProductsInstalled are having the highest percentage of malware detected.
  ### 3.3.1 KDE and Box plot for numerical columns
  
  ![kde](https://github.com/vinaydalu/Microsoft_Malware_Prediction/blob/main/Images/AVP_product.png)
  
  ### 3.3.2 swarm plot, violin plot, strip plot for numerical columns
  
  ![plot](https://github.com/vinaydalu/Microsoft_Malware_Prediction/blob/main/Images/violin.png)
  
  # <u> 4. Preprocessing
  We have total 75 columns in our data, out of 75,26 are categorical data . Machine learning algorithms can’t work with categorical data. So we need to perform some encoding techniques and some feature extraction techniques  on our data before we start modeling .
There are different encoding techniques for converting categorical columns into numerical columns like:
  
1.One-Hot Encoding.
  
2.Label Encoding.
  
3.Target Encoding.
  
4.Binary Encoding etc
  ## <u> 4.1 Why Target Encoding ?
  Target Encoding is a process of replacing a categorical value with the mean of the target variable.Any non-categorical columns are automatically dropped by the target encoder model. 
  
Note : It also helps to convert categorical value to numeric which helps to improve machine learning accuracy since algorithms have a hard time dealing with high cardinality columns.
  
* For this dataset in each categorical column we have more number of categories. So while applying OneHot encoding we will face the problem of Curse of Dimensionality. 
* In order to use Label Encoding the values of the features should be ordinal. But in this dataset the data is not ordinal. 
* In order to apply Binary Encoding the column should contain atmost 2 categories.
## <u> 4.2 Weight Of Evidance(WOE) And Information Value(IV)
  It is one of the techniques used for variable selection. WOE is used to encode categorical variables for classification and IV helps to determine which columns in a dataset have a predictive power.
  
  ![woe](https://github.com/vinaydalu/Microsoft_Malware_Prediction/blob/main/Images/weight%20of%20evidence.png)
  
  We use WOE values rather than raw categories in our model. The transformed variable will be a continues with WOE values. It is same as any continuous variable.
  
  Iv is one of the most useful technique to select important variables in a predictive model . It helps to rank the variables basis on their importance.
  
  ![iv](https://github.com/vinaydalu/Microsoft_Malware_Prediction/blob/main/Images/IV.png)
  
  Based on the IV score we select the important variable by using some rules.
  
  ![table](https://github.com/vinaydalu/Microsoft_Malware_Prediction/blob/main/Images/IV%20table.png)
  
  After performing WOE & IV we consider top 15 features , only one feature is medium predictor (SmartScreen_target) and all are weak predictors which doesn't provide the sufficient predictor variables. So we try implementing other feature extraction techniques.
  ## <u> 4.3 Recursive Feature Elimination(RFE)
  RFE is one of the popular feature selection algorithm because of it’s configure and use and because it is effective in selecting those features in a training dataset that are more relevant in predicting the target variable(HasDetections).
There are two important configuration options when using RFE :
* The choice in the number of features to select i.e, n_features_to_select =10
* The choice of the algorithm used to help choose features i.e, estimator=logreg.
* The RFE that select 10 features and then select decision tree on selected features achieves an accuracy of about 56%.
## <u> 4.4 Feature Engineering
  Feature Engineering is a process of using domain knowledge to extract features from raw data which can be used to enhance the performance of machine learning algorithms.
### 4.4.1 Feature Engineering using Feature Tools
  Feature tools is used to perform automated feature engineering. 
* We will create an EntitySet that contains multiple dataframes and relations between them and add the dataframe combination to it.
* Now we use Deep Feature Synthesis (DFS) which uses feature primitives(here we use addition,multiplication) to create features using multiple tables present in the EntitySet.
* By using feature tools we have automated the process of creating features but seeing the dimensions of acquired dataframe, we got 2235 features which leads to a curse of Dimensionality.
## <u> 4.5 Principal Component of Analysis (PCA)
  PCA is a dimensionality-reduction method that is often used to reduce the dimensionality of large datasets , by transforming a large set of variables into smaller ones that still contains most of the information in the large set.

First of all , we standardize the range of continuous initial variables so that each one of them contributes equally to the analysis. Mathematically, this can be done by subtracting the mean and dividing by the standard deviation for each value of each variable.

  ![formula](https://github.com/vinaydalu/Microsoft_Malware_Prediction/blob/main/Images/standardization.png)
  
  Once the standardization is done, all the variables will be transformed to the same scale.
  Sometimes variables are highly correlated in such a way that they contain redundant information. So, in order to identify these correlations, we compute the covariance matrix.
  
  The covariance matrix is a p x p symmetric matrix where p is the number of dimensions that has entries the covariances associated with all possible pairs of the initial variables.Since the covariance of a variable with itself is its variance (Cov(a,a)=Var(a)), in the main diagonal (Top left to bottom right) we actually have the variances of each initial variable. And since the covariance is commutative (Cov(a,b)=Cov(b,a)), the entries of the covariance matrix are symmetric with respect to the main diagonal, which means that the upper and the lower triangular portions are equal.

It’s actually the signs of covariance that matters,

If positive then : the two variables increase or decrease (correlated)
If negative then : one increase and the other decrease (inversely correlated) 
  
Eigenvectors and Eigenvalues are the linear algebra concepts that we need to compute from the covariance matrix in order to determine principal components.
PCA tries to put maximum possible information in the first component, then maximum remaining information in the second and so on...
  
  ![comp](https://github.com/vinaydalu/Microsoft_Malware_Prediction/blob/main/Images/principle%20components.png)
  
  Organizing information in this way, will allow to reduce dimensionality without losing much information and this by discarding components with low information and considering the remaining components as new variables.
  
  ![pca](https://github.com/vinaydalu/Microsoft_Malware_Prediction/blob/main/Images/PCA.png)
  
  By the above graph it’s clear that the most variance showing features are between 30-40 . So we can choose between these range for the n_components.
  Finally, we choose whether to keep all the components or discard those of lesser significance and forms with remaining ones a matrix of vectors that we call Feature vector.
# <u> 5. Data Modeling
  As we know ,for modeling we need to convert all features to numeric as most of the ML models won’t take categorical variables. I thought using logistic algorithm would be good start for modeling and for conversion we tried RFE which gave us ranks for the features . we consider taking upto rank 10 features for modeling. 
## <u> 5.1 Logistic regression 
We have total 19 features which have ranks upto 10 and on applying logistic regression algorithm we got about an accuracy of 55% which is not a satisfactory accuracy . So we tried to improve our performance by doing hyperparameter optimization/tuning .
  
The first thought we have when we heard of hyperparameter i.e., which set of hyperparameters will give us best result? That’s when GridSearchCV will comes into the picture , which helps to  identify the best fit parameters for our model.
  
After doing hyperparameter tuning we got accuracy of 57% . Increased by 2% , which seems good but wasn’t good enough. So we tried some other algorithms which will give us satisfactory accuracy.
  ## <u> 5.2 Decision Tree 
After applying Decision tree model on our data including all feature we got an accuracy of 56%. Including only important features may improve the accuracy of our model. With the help of Feature Importance , we are going to calculate the importance of a feature.
  
Feature importance is a technique that assign a score to input features based on how useful they are at predicting a target variable. We are considering the features which has score > 0.01
  
Now we perform decision tree classifier on our data with these important features and got accuracy 56% which is same as before. So we do parameter tuning for our model  and the accuracy was improved significantly from 56 to 60%. 
  ### 5.2.1 AUC-ROC score and curve for HasDetections = 1
  
![roc](https://github.com/vinaydalu/Microsoft_Malware_Prediction/blob/main/Images/decision%20tree_ROC.png)
  
No Skill: ROC AUC=0.500
  
Decision Tree: ROC AUC=0.667
## <u> 5.3 Random Forest
With including the all feature we apply random forest and got an accuracy of 63% which is a pretty good score. Now we try with only important feature by using Feature importance which may give us a better performance. The process is same which we use in Decision tree model, considering the features which have score > 0.01 .
With the important features we apply GridSearchCV to find the best parameters and after parameter tuning we got the same accuracy of 63%.
### 5.3.1 AUC-ROC score and curve for HasDetections = 1

  ![roc](https://github.com/vinaydalu/Microsoft_Malware_Prediction/blob/main/Images/random%20forest_ROC.png)
  
No Skill: ROC AUC=0.500
  
Random Forest: ROC AUC=0.702

In random forest , after we carefully tune parameters still we got the same accuracy  for our model. But still we accomplished 63% of accuracy which is a pretty good score . I thought this is a good process and start searching for better models which will give us a better performance and i come up across the XGBoost .
## <u> 5.4 XGBoost classifier
After performing XGBoost classifier we got an accuracy of 64% in which we include all features in our model. Using feature importance we extract the features which are useful at predicting a target variable.
After parameter tuning, the training set accuracy increased by 4% but testing accuracy remains same as 64%.
 ## <u> 5.5 Bayesian optimization using Hyperopt and optuna
 Then hyperparameter optimization is a process of finding the right combination of hyperparameter values in order to achieve maximum performance on the data in a reasonable amount of time.some common strategies for optimizing hyperparameters are GridSearch and Random search.
  
Bayesian Optimisation, can bring down the time spent to get to the optimal set of parameters and bring better generalisation performance on the test set.
  
So in order to overcome this drawback we gonna try to implement some alternative hyperparameter optimization techniques.
 ### 5.5.1 Hyperopt

Hyperopt uses a form of Bayesian optimization for parameter tuning that allows you to get the best parameters for a given model. It can optimize a model with hundreds of parameters on a large scale.

### 5.5.2 Optuna

Optuna is a software framework for automating the optimization process of these hyperparameters. It automatically finds optimal hyperparameter values by making use of different samplers such as grid search, random, bayesian, and evolutionary algorithms.
## <u> 5.6 LightGBM
  LightGBM is a fast, distributed, high performance gradient boosting framework based on decision tree algorithms, used for ranking, classification and many other machine learning tasks.
  
Without doing parameter tuning we got an accuracy of 65%. Using Hyperopt we extract the best features. When we use optuna for the optimization we got accuracy of 65%. Using bayesian optimization we got 66%.
  
 ### 5.6.1 LightGBM on PCA(Bayesian optimization)
  Using PCA we extract the best featues for our model and for best fit parameters we use hyperopt optimization. Finally we implement LightGBM model and got an accuracy of 63%.
  ## ML Algorithms and their metrics
  The following table will get an insight of models used and accuracies for each of them.It is understood that LightGBM was faster and more accurate among all the other models
  
  ![table](https://github.com/vinaydalu/Microsoft_Malware_Prediction/blob/main/Images/pred_table.png)
  
  But, we have a set of parameters to be provided to use LightGBM. This hyperparameter tuning needed to be done and made use of Bayesian optimiser hyper parameter tuning
and got the best parameters for the LightGBM model.It's a very different dataset which has very less correlation in the kaggle competition itself the highest accuracy was 69% reaching 66% is not a simple task finally we have achieved using very less sample size data and getting this much accuracy .
  
  # <u> 6. Future Improvements
  * I used machine learning algorithms but if possible i would like to use Deep learning algorithms like ANN,CNN,LSTM for future implements and want to check for any accuracy improvement.
  * Did use other models like catboost but didn’t dig deep as we can better results in other models. Taking up from there looks good.
  
  # <u> 7. Reffered Links
  * https://towardsdatascience.com/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f
  
  * https://www.analyticsvidhya.com/blog/2020/09/alternative-hyperparameter-optimization-technique-you-need-to-know-hyperopt/
  
  * https://towardsdatascience.com/how-to-make-your-model-awesome-with-optuna-b56d490368af
  
  * https://www.kaggle.com/c/microsoft-malware-prediction
  
  * https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMClassifier.html
  
  * https://stackoverflow.com/questions/49170296/scikit-learn-feature-importance-calculation-in-decision-trees
  
  












  

